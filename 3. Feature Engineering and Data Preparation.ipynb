{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "570584b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # for array\n",
    "import pandas as pd  # for csv files and dataframe\n",
    "import matplotlib.pyplot as plt  # for plotting\n",
    "import seaborn as sns  # plotting\n",
    "from scipy import stats\n",
    "\n",
    "import pickle  # To load data int disk\n",
    "from prettytable import PrettyTable  # To print in tabular format\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, make_scorer\n",
    "from sklearn.metrics import auc, f1_score, roc_curve\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37ec0a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train_alldata_EDA.csv')\n",
    "test = pd.read_csv('./test_alldata_EDA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08fa4362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function\n",
    "def multi_corr(col1, col2=\"label\", df=train):\n",
    "    '''\n",
    "    This function returns correlation between 2 given features.\n",
    "    Also gives corr of the given features with \"label\" afetr applying log1p to it.\n",
    "    '''\n",
    "    corr = df[[col1, col2]].corr().iloc[0,1]\n",
    "    log_corr = df[col1].apply(np.log1p).corr(df[col2])\n",
    "\n",
    "    print(\"Correlation : {}\\nlog_Correlation: {}\".format(corr, log_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "954f04dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(col1, col2=\"label\", df=train):\n",
    "    \"\"\"\n",
    "    This function returns correlation between 2 given features\n",
    "    \"\"\"\n",
    "    return df[[col1, col2]].corr().iloc[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8081c6",
   "metadata": {},
   "source": [
    "### Removing highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f339620e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting all the features with high correlation values with other features\n",
    "# Refer: https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/\n",
    "corr_matrix = train.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.9\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e11e2077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sloss', 'dloss', 'dpkts', 'dwin', 'ltime', 'ct_srv_dst', 'ct_src_dport_ltm', 'ct_dst_src_ltm']\n"
     ]
    }
   ],
   "source": [
    "# We don't want to use these features for plotting because these are having high corr\n",
    "# And most likely have same kind of plots with already plotted feature\n",
    "print(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d46af3d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'saved_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_85924/2488407337.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msaved_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'corr_col'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_drop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'saved_dict' is not defined"
     ]
    }
   ],
   "source": [
    "saved_dict['corr_col'] = to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044334e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the features from train and test data\n",
    "train.drop(columns=to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f64b0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4747058c",
   "metadata": {},
   "source": [
    "### Adding New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e280aa",
   "metadata": {},
   "source": [
    "Refer: https://www.elastic.co/guide/en/ecs/master/ecs-network.html\n",
    "\n",
    "Network bytes: Total bytes trasferred by the network. It is sum of 'sbytes' (Source to destination bytes) and 'dbytes' (Destination to source bytes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6a5900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new features\n",
    "train['network_bytes'] = train['sbytes'] + train['dbytes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99138b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eebf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns which are not useful for the classification\n",
    "# attack_cat is for multiclass classification\n",
    "# all the other columns are address related and not present in sample train data\n",
    "train.drop(['srcip', 'sport', 'dstip', 'dsport', 'attack_cat'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bdf8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use during test data transformation\n",
    "saved_dict['to_drop'] = ['srcip', 'sport', 'dstip', 'dsport', 'attack_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd764be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a659cce6",
   "metadata": {},
   "source": [
    "### Applying log1p on Numerical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef6009f",
   "metadata": {},
   "source": [
    "During EDA we found that few numerical columns shows better visualization for pdf curves if we apply log1p to the columns.\n",
    "\n",
    "So I thought to try log1p on all the columns and check the correlation value of the original column and log1p column with target column i.e. \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fae26ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting number of unique values of all the columns\n",
    "# If the unique values are high that means it has continuous set of values\n",
    "col_unique_values = train.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9244e63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the unique values are getter than some threshould than we will check its corr\n",
    "col = col_unique_values[col_unique_values>200].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5dc03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking corr value of original col and log1p applied col\n",
    "# Taking those columns whose unique values are getter than some threshould\n",
    "for column in col:\n",
    "    print(\"{:-^30}\".format(column))\n",
    "    multi_corr(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2964b618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will apply log1p on this columns and remove original columns\n",
    "log1p_col = ['dur', 'sbytes', 'dbytes', 'sload', 'dload', 'spkts', 'stcpb', 'dtcpb', 'smeansz', 'dmeansz', 'sjit', 'djit', 'network_bytes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72fac65",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_dict['log1p_col'] = log1p_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3150f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode values of every features, will use to fill Null values of test\n",
    "mode_dict = train.mode().iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bf2742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log1p_transform(col, df=train):\n",
    "    '''\n",
    "    Apply log1p on given column.\n",
    "    Remove the original cola and keep log1p applied col\n",
    "    '''\n",
    "    new_col = col+'_log1p'\n",
    "    df[new_col] = df[col].apply(np.log1p)\n",
    "    df.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ad4951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming columns with log1p\n",
    "for col in log1p_col:\n",
    "    log1p_transform(col, df=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9278643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ca69a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c4bdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aabf48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating x and y set from the dataset\n",
    "x_train, y_train = train.drop(columns=['label']), train['label']\n",
    "x_test, y_test = test.drop(columns=['label']), test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4cbb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print()\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf79bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving all the files to disk to use later\n",
    "pickle.dump((x_train, y_train), open('.final_ipynb/final_train.pkl', 'wb'))\n",
    "pickle.dump((x_test, y_test), open('.final_ipynb/final_test.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055b8de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting categorical and numerical columns in 2 diff lists\n",
    "cat_col = ['proto', 'service', 'state']\n",
    "num_col = list(set(x_train.columns) - set(cat_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdd5620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use later, during test data cleaning\n",
    "saved_dict['cat_col'] = cat_col\n",
    "saved_dict['num_col'] = num_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9ecb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d82fcb7",
   "metadata": {},
   "source": [
    "### Standardizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549dd7ea",
   "metadata": {},
   "source": [
    "As we have seen that the range of few features in this dataset is very large. So we will keep everything within certain range by applying standardscaler. After this all the features will have mean 0 and std 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d870d550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the data\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(x_train[num_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7437f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[num_col] = scaler.transform(x_train[num_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4c2eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c4114b",
   "metadata": {},
   "source": [
    "### Onehot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eedfb12",
   "metadata": {},
   "source": [
    "In our dataset we have few categorical columns with text data. But ML models can't process text data it can process numbers.\n",
    "\n",
    "So we have to convert categorical columns to numerical columns in some way. We will use onehotencoder where we will assign 1 if the value is present for the row and rest of the columns will be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6615235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Onehot Encoding\n",
    "service_ = OneHotEncoder()\n",
    "proto_ = OneHotEncoder()\n",
    "state_ = OneHotEncoder()\n",
    "ohe_service = service_.fit(x_train.service.values.reshape(-1,1))\n",
    "ohe_proto = proto_.fit(x_train.proto.values.reshape(-1,1))\n",
    "ohe_state = state_.fit(x_train.state.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead2573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are onehot encoding the given column\n",
    "# Remove the original categorical column\n",
    "for col, ohe in zip(['proto', 'service', 'state'], [ohe_proto, ohe_service, ohe_state]):\n",
    "    x = ohe.transform(x_train[col].values.reshape(-1,1))\n",
    "    tmp_df = pd.DataFrame(x.todense(), columns=[col+'_'+i for i in ohe.categories_[0]])\n",
    "    x_train = pd.concat([x_train.drop(col, axis=1), tmp_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305c9a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea01e265",
   "metadata": {},
   "source": [
    "Saving all the important parameters and objects to disk so that we can apply same process on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22803c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'final_ipynb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d94fe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(scaler, open(file_path+'scaler.pkl', 'wb'))  # Standard scaler\n",
    "pickle.dump(saved_dict, open(file_path+'saved_dict.pkl', 'wb'))  # Dictionary with important parameters\n",
    "pickle.dump(mode_dict, open(file_path+'mode_dict.pkl', 'wb'))  #  Dictionary with most frequent values of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8d50ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Onehot encoder for categorical columns\n",
    "pickle.dump(ohe_proto, open(file_path+'ohe_proto.pkl', 'wb'))\n",
    "pickle.dump(ohe_service, open(file_path+'ohe_service.pkl', 'wb'))\n",
    "pickle.dump(ohe_state, open(file_path+'ohe_state.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87733138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned and processed train data\n",
    "pickle.dump((x_train, y_train), open(file_path+'final_train.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001b88be",
   "metadata": {},
   "source": [
    "### Pipeline functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f71893",
   "metadata": {},
   "source": [
    "We have to prepare a pipeline, where we can send raw data and get the output.\n",
    "\n",
    "We will use test data to implement the pipeline. Here we will use all the parameters we have saved using train data.\n",
    "\n",
    "Also standardize and onehot encode test data using train data objects for standardscaler and onehotencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c1f05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    '''\n",
    "    Cleans given raw data. Performs various cleaning, removes Null and wrong values.\n",
    "    Check for columns datatype and fix them.\n",
    "    '''\n",
    "    numerical_col = data.select_dtypes(include=np.number).columns  # All the numerical columns list\n",
    "    categorical_col = data.select_dtypes(exclude=np.number).columns  # All the categorical columns list\n",
    "    \n",
    "    # Cleaning the data\n",
    "    for col in data.columns:\n",
    "        val = mode_dict[col]  # Mode value of the column in train data\n",
    "        data[col] = data[col].fillna(value=val)\n",
    "        data[col] = data[col].replace(' ', value=val)\n",
    "        data[col] = data[col].apply(lambda x:\"None\" if x==\"-\" else x)\n",
    "\n",
    "        # Fixing binary columns\n",
    "        if col in saved_dict['binary_col']:\n",
    "            data[col] = np.where(data[col]>1, val, data[col])\n",
    "\n",
    "    # Fixing datatype of columns\n",
    "    bad_dtypes = list(set(categorical_col) - set(saved_dict['cat_col']))\n",
    "    for bad_col in bad_dtypes:\n",
    "        data[col] = data[col].astype(float)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9ce6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_log1p(data):\n",
    "    '''\n",
    "    Performs FE on the data. Apply log1p on the specified columns create new column and remove those original columns.\n",
    "    '''\n",
    "    for col in saved_dict['log1p_col']:\n",
    "        new_col = col + '_log1p'  # New col name\n",
    "        data[new_col] = data[col].apply(np.log1p)  # Creating new column on transformed data\n",
    "        data.drop(col, axis=1, inplace=True)  # Removing old columns\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e050f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(data):\n",
    "    '''\n",
    "    Stanardize the given data. Performs mean centering and varience scaling.\n",
    "    Using stanardscaler object trained on train data.\n",
    "    '''\n",
    "    data[saved_dict['num_col']] = scaler.transform(data[saved_dict['num_col']])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f5fceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohencoding(data):\n",
    "    '''\n",
    "    Onehot encoding the categoricla columns.\n",
    "    Add the ohe columns with the data and removes categorical columns.\n",
    "    Using Onehotencoder objects trained on train data.\n",
    "    '''\n",
    "\n",
    "    # Onehot encoding cat col using onehotencoder objects\n",
    "    X = ohe_service.transform(data['service'].values.reshape(-1, 1))\n",
    "    Xm = ohe_proto.transform(data['proto'].values.reshape(-1, 1))\n",
    "    Xmm = ohe_state.transform(data['state'].values.reshape(-1, 1))\n",
    "    \n",
    "    # Adding encoding data to original data\n",
    "    data = pd.concat([data,\n",
    "                      pd.DataFrame(Xm.toarray(), columns=['proto_'+i for i in ohe_proto.categories_[0]]),\n",
    "                      pd.DataFrame(X.toarray(), columns=['service_'+i for i in ohe_service.categories_[0]]),\n",
    "                      pd.DataFrame(Xmm.toarray(), columns=['state_'+i for i in ohe_state.categories_[0]])],\n",
    "                      axis=1)\n",
    "    \n",
    "    # Removing cat columns\n",
    "    data.drop(['proto', 'service', 'state'], axis=1, inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c83028d",
   "metadata": {},
   "source": [
    "Loading all the objects from disk, that we have trained on train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d5f7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametrs\n",
    "saved_dict = pickle.load(open(file_path+'saved_dict.pkl', 'rb'))\n",
    "# Mode value of all the columns\n",
    "mode_dict = pickle.load(open(file_path+'mode_dict.pkl', 'rb'))\n",
    "# Stanardscaler object\n",
    "scaler = pickle.load(open(file_path+'scaler.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df79eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoder objects\n",
    "ohe_proto = pickle.load(open(file_path+'ohe_proto.pkl', 'rb'))\n",
    "ohe_service = pickle.load(open(file_path+'ohe_service.pkl', 'rb'))\n",
    "ohe_state = pickle.load(open(file_path+'ohe_state.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86897080",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b02269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting index of test data\n",
    "x_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca53384",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7805da",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ed550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding column names\n",
    "x_test.columns = saved_dict['columns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafad2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new Feature\n",
    "x_test['network_bytes'] = x_test['dbytes'] + x_test['sbytes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b709b80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping all the unwanted columns\n",
    "dropable_col = saved_dict['to_drop'] + saved_dict['corr_col']\n",
    "x_test.drop(columns=dropable_col, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b914298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9145e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning data using clean_data()\n",
    "x_test = clean_data(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd06cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8021eee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FE: applying log1p using apply_log1p()\n",
    "x_test = apply_log1p(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e60d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91542224",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b7b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardscaling using stanardize()\n",
    "x_test = standardize(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cc4f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b279343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Onehot encoding categorical columns using ohencoding()\n",
    "x_test = ohencoding(x_test)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c85e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b2f311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final test data\n",
    "x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba99cd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching test data columns with train data columns\n",
    "all(x_train.columns == x_test.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
